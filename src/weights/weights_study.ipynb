{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "import requests\n",
    "import cvxpy as cp\n",
    "\n",
    "import time\n",
    "import traceback\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_pool_data(query, network, include=None, page=1):\n",
    "    # Define the base URL\n",
    "    base_url = \"https://api.geckoterminal.com/api/v2/search/pools\"\n",
    "\n",
    "    # Define the query parameters\n",
    "    params = {\n",
    "        'query': query,\n",
    "        'network': network,\n",
    "        'page': page\n",
    "    }\n",
    "\n",
    "    # Add the 'include' parameter if it was provided\n",
    "    if include is not None:\n",
    "        params['include'] = include\n",
    "\n",
    "    # Make the HTTP request\n",
    "    response = requests.get(base_url, params=params)\n",
    "\n",
    "    # Check for a successful response\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"API request failed with status code {response.status_code}\")\n",
    "\n",
    "    # Parse the JSON response and extract the 'data' data\n",
    "    data = response.json()['data']\n",
    "\n",
    "    # Prepare a list to store the data for the DataFrame\n",
    "    df_data = []\n",
    "\n",
    "    # Loop through each item in the data\n",
    "    for item in data:\n",
    "        # Extract the attributes\n",
    "        attributes = item['attributes']\n",
    "\n",
    "        # Prepare a dictionary to store the data for this row\n",
    "        row = {\n",
    "            'id': item['id'],\n",
    "            'address': attributes['address'],\n",
    "            'name': attributes['name'],\n",
    "            'fdv_usd': attributes['fdv_usd'],\n",
    "            'market_cap_usd': attributes['market_cap_usd'],\n",
    "        }\n",
    "\n",
    "        # Add the row to the list of data for the DataFrame\n",
    "        df_data.append(row)\n",
    "\n",
    "    # Convert the data to a pandas DataFrame\n",
    "    df = pd.DataFrame(df_data)\n",
    "\n",
    "    return df\n",
    "\n",
    "def fetch_ohlc_data(network, pool_address, timeframe, aggregate=1, before_timestamp=None, limit=1000):\n",
    "    # Define the base URL\n",
    "    base_url = \"https://api.geckoterminal.com/api/v2/networks/{network}/pools/{pool_address}/ohlcv/{timeframe}\"\n",
    "\n",
    "    # Format the URL with the provided parameters\n",
    "    url = base_url.format(network=network, pool_address=pool_address, timeframe=timeframe)\n",
    "\n",
    "    # Define the query parameters\n",
    "    params = {\n",
    "        'aggregate': aggregate,\n",
    "        'limit': limit\n",
    "    }\n",
    "\n",
    "    # Add the 'before_timestamp' parameter if it was provided\n",
    "    if before_timestamp is not None:\n",
    "        params['before_timestamp'] = before_timestamp\n",
    "\n",
    "    # Make the HTTP request\n",
    "    response = requests.get(url, params=params)\n",
    "\n",
    "    # Check for a successful response\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"API request failed with status code {response.status_code}\")\n",
    "\n",
    "    # Parse the JSON response and extract the 'ohlcv_list' data\n",
    "    data = response.json()['data']['attributes']['ohlcv_list']\n",
    "\n",
    "    # Convert the data to a pandas DataFrame\n",
    "    df = pd.DataFrame(data, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])\n",
    "    \n",
    "    # Convert timestamp to datetime\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'], unit='s')\n",
    "    \n",
    "    #sort by timestamp\n",
    "    df = df.sort_values(by=['timestamp']).reset_index(drop=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def Follow_The_Quadratized_Leader_weights(r,df,epsilon=0):\n",
    "  T = r.shape[0]\n",
    "  n = r.shape[1]\n",
    "  if T==0:\n",
    "    return pd.DataFrame(np.ones((1, n))/n, columns=df.columns)\n",
    "  \n",
    "  x = np.zeros((T,n))\n",
    "  x_t_plus_1 = np.ones(n)/n\n",
    "  b_t = 0\n",
    "  A_t=1e-3*np.eye(n)\n",
    "  for t in range(T):\n",
    "    x[t] = x_t_plus_1\n",
    "\n",
    "    A_t += np.outer(r[t],r[t])/np.dot(r[t],x[t])\n",
    "    b_t += -r[t]/np.dot(r[t],x[t]) - r[t]\n",
    "\n",
    "    var_x = cp.Variable(n)\n",
    "    \n",
    "\n",
    "    obj = 0.5* cp.quad_form(var_x,  A_t) + b_t.T@var_x + epsilon * 0.5 * cp.sum_squares(var_x)\n",
    "    prob = cp.Problem(cp.Minimize(obj), [cp.sum(var_x) == 1, var_x >= 0])\n",
    "    prob.solve()\n",
    "    x_t_plus_1 = var_x.value\n",
    "\n",
    "  return pd.DataFrame(x, columns=df.columns, index=df.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GUAC_1hour.csv saved\n",
      "HXD_1hour.csv saved\n",
      "LIKE_1hour.csv saved\n",
      "NOS_1hour.csv saved\n",
      "PRNT_1hour.csv saved\n",
      "SAMO_1hour.csv saved\n",
      "SLIM_1hour.csv saved\n",
      "SNS_1hour.csv saved\n",
      "STARS_1hour.csv saved\n",
      "STEP_1hour.csv saved\n",
      "BONK_1hour.csv saved\n",
      "PUMPR_1hour.csv saved\n",
      "RAY_1hour.csv saved\n",
      "WIF_1hour.csv saved\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'{\"1703318400000\":{\"GUAC\":0.0,\"HXD\":0.0,\"LIKE\":0.0,\"NOS\":0.0,\"PRNT\":0.0,\"SAMO\":0.0,\"SLIM\":0.0,\"SNS\":0.0,\"STARS\":0.0,\"STEP\":0.0,\"BONK\":0.0,\"PUMPR\":0.0,\"RAY\":0.0,\"$WIF\":1.0,\"USDC\":0.0}}'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def main():\n",
    "\n",
    "    merged_df = None # Initialize merged_df to concatenate all the data\n",
    "\n",
    "    aggregate = 1 #15\n",
    "    timeframe = 'hour'#'minute'\n",
    "    network = 'solana'\n",
    "    \n",
    "    # Determine the root directory\n",
    "    root_dir = os.path.dirname(os.getcwd())\n",
    "\n",
    "    # Construct the path to the JSON file\n",
    "    json_file_path = os.path.join(root_dir, '../whitelist.json')\n",
    "\n",
    "    # Read the JSON file\n",
    "    with open(json_file_path, 'r') as f:\n",
    "        symbols = json.load(f)\n",
    "\n",
    "    # Remove 'WSOL' and 'USDC' from the list\n",
    "    filtered_symbols = [symbol for symbol in symbols if symbol not in ['WSOL', 'USDC']]\n",
    "    tokens_with_sol_pairs = filtered_symbols.copy()\n",
    "\n",
    "    #tokens_with_sol_pairs = ['GUAC', 'HXD','LIKE', 'NOS', 'PRNT', 'SAMO', 'SLIM', 'SNS', 'STARS', 'STEP','BONK','RAY']\n",
    "    symbols = tokens_with_sol_pairs.copy()\n",
    "\n",
    "    for symbol in tokens_with_sol_pairs:\n",
    "        token_df = fetch_pool_data(query= symbol, network=network, include='base_token,quote_token', page=1)\n",
    "\n",
    "        pool_address = token_df['address'].iloc[0]\n",
    "        pair_name = token_df['name'].iloc[0] # GUAC/USDC helps identify the quote token\n",
    "\n",
    "        df = fetch_ohlc_data(network, pool_address, timeframe, aggregate=aggregate, before_timestamp=None, limit=1000)\n",
    "        df['pair'] = pair_name\n",
    "        \n",
    "        directory = f\"assets/{network}/{symbol}\"\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "        df.to_csv(f'{directory}/{symbol}_{aggregate}{timeframe}.csv', index=False)\n",
    "\n",
    "        print(f'{symbol}_{aggregate}{timeframe}.csv saved')\n",
    "    \n",
    "    for symbol in symbols:\n",
    "        try:    \n",
    "            file_path = f'assets/solana/{symbol}/{symbol}_{aggregate}{timeframe}.csv'\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "            # Ensure proper timestamp parsing\n",
    "            df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "            df = df.set_index('timestamp')\n",
    "            symbol = df['pair'].iloc[0].replace(' ', '').replace('/', '-')\n",
    "\n",
    "            # Keep only the 'close' column and rename it\n",
    "            df = df[['close']].rename(columns={'close': symbol})\n",
    "            \n",
    "            if merged_df is None:\n",
    "                merged_df = df\n",
    "            else:\n",
    "                # Merge using outer join to keep all timestamps\n",
    "                merged_df = pd.merge(merged_df, df, left_index=True, right_index=True, how='outer')\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f'No data or error for {symbol}: {str(e)}')\n",
    "            continue\n",
    "\n",
    "    if merged_df is not None:  # Check if merged_df is not None before using it\n",
    "        merged_df.reset_index(inplace=True)\n",
    "        merged_df.dropna(inplace=True)\n",
    "        merged_df.set_index('timestamp', inplace=True)\n",
    "\n",
    "        #adding USDC stablecoin to the dataframe\n",
    "        merged_df['USDC'] = 1.0\n",
    "\n",
    "        df = 1+merged_df.pct_change().dropna()/100\n",
    "        r = df.to_numpy()\n",
    "        df = df.cumprod(axis=0)\n",
    "        \n",
    "        weights_df = Follow_The_Quadratized_Leader_weights(r,df, epsilon=0)\n",
    "        weights_df = weights_df.round(3).abs()\n",
    "\n",
    "        # Split the column names on '-' and keep only the first part (the base symbol)\n",
    "        weights_df.columns = [name.split('-')[0] for name in weights_df.columns]\n",
    "        \n",
    "        if len(weights_df) >= 2:\n",
    "            changes = weights_df.iloc[-1] - weights_df.iloc[-2]\n",
    "            assets_to_buy = changes[changes > 0].round(3)\n",
    "            assets_to_sell = changes[changes < 0].abs().round(3)\n",
    "        else:\n",
    "            print(\"weights_df has less than 2 rows. Skipping this iteration.\")\n",
    "\n",
    "        # Convert the Series into lists of dictionaries\n",
    "        assets_to_buy_list = [{'symbol': symbol, 'weight': weight} for symbol, weight in assets_to_buy.items()]\n",
    "        assets_to_sell_list = [{'symbol': symbol, 'weight': weight} for symbol, weight in assets_to_sell.items()]\n",
    "\n",
    "        recent_weights_df = weights_df.tail(1)\n",
    "        recent_weights_json = recent_weights_df.tail(1).to_json(orient='index')\n",
    "        #recent_weights_df.tail(1).to_json(f'recent_weights.json', orient='index')\n",
    "\n",
    "        # Save the lists to JSON files\n",
    "        with open('buy.json', 'w') as f:\n",
    "            json.dump(assets_to_buy_list, f)\n",
    "\n",
    "        with open('sell.json', 'w') as f:\n",
    "            json.dump(assets_to_sell_list, f)        \n",
    "\n",
    "        # Get the last two rows\n",
    "        last_two_rows = weights_df.tail(2)\n",
    "        # Convert the timestamp to your timezone (Germany)\n",
    "        last_two_rows.index = last_two_rows.index.tz_localize('UTC').tz_convert('Europe/Paris')\n",
    "        #print(tabulate(last_two_rows, headers='keys', tablefmt='psql', showindex=True))\n",
    "        return recent_weights_json\n",
    "\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trading",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
